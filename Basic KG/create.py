import json
import pandas as pd
import argparse
import spacy
import networkx as nx
from difflib import get_close_matches
from langchain.document_loaders import PyPDFLoader
import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
import webbrowser
from sentence_transformers import SentenceTransformer, util
import plotly.graph_objects as go
from langchain_groq import ChatGroq
from dotenv import load_dotenv
from langchain_core.runnables import RunnablePassthrough

# Load spaCy model for English (optional if needed for fallback)
nlp = spacy.load("en_core_web_sm")
load_dotenv()

import re

def parse_triples(response):
    """
    Parse the response from the LLM into a list of triples.
    Handles both JSON and plain-text formats.
    """
    triples = []
    try:
        # Attempt to parse JSON response
        triples = json.loads(response.content)
    except json.JSONDecodeError:
        # If the response is not JSON, fallback to plain text parsing
        lines = response.content.split("\n")
        
        # Regex pattern to capture (head, relation, tail) in plain-text format
        pattern = r"\(([^,]+),\s*([^,]+),\s*([^)]+)\)"
        
        for line in lines:
            line = line.strip()
            match = re.search(pattern, line)
            if match:
                head, relation, tail = match.groups()
                triples.append((head.strip(), relation.strip(), tail.strip()))
            else:
                print(f"Skipping malformed line: {line}")
    except Exception as e:
        print(f"Error parsing triples from LLM response: {e}")

    return triples

def transform_corpus_to_knowledge_graph(corpus_text):
    """
    Transform the corpus into a knowledge graph by querying an LLM (Groq).
    The LLM will generate triples (head, relation, tail) representing knowledge from the text.
    """
    # Initialize the LLM (Groq)
    llm = ChatGroq(
        model="mixtral-8x7b-32768",  
        temperature=0.5,
        max_tokens=1024,
    )

    # Prepare the prompt for extracting triples
    prompt = f"""
    You are a knowledge extraction assistant. Given the following corpus of text, extract the relationships between entities and provide them in the form of triples (head, relation, tail).
    
    Corpus:
    {corpus_text}
    
    Please list all the triples in the following format: (head, relation, tail).
    """

    # Generate the response from the LLM
    response = llm.invoke(prompt)

    # Parse the response to extract triples
    triples = parse_triples(response)

    return triples


def build_knowledge_graph_from_llm(triples):
    """
    Build a schemaless knowledge graph from the triples generated by the LLM.
    """
    G = nx.DiGraph()

    # Check the format of triples before attempting to unpack
    for triple in triples:
        if isinstance(triple, tuple) and len(triple) == 3:
            head, relation, tail = triple
        elif isinstance(triple, dict):
            # If the triple is a dictionary (e.g., {head: relation, tail: ...})
            head = triple.get('head')
            relation = triple.get('relation')
            tail = triple.get('tail')
        else:
            # Handle unexpected format or log it
            print(f"Unexpected format: {triple}")
            continue

        # Normalize text and add to graph
        head_normalized = normalize_text(head)
        tail_normalized = normalize_text(tail)

        # Check if the edge is being added correctly
        print(f"Adding edge: {head_normalized} -[{relation}]-> {tail_normalized}")
        G.add_edge(head_normalized, tail_normalized, label=relation)

    # Return the graph
    return G


def normalize_text(text):
    """
    Normalize text by lowercasing and stripping special characters.
    """
    return text.lower().strip()

def extract_query_concepts(query):
    """
    Extract key concepts from the query using spaCy dependency parsing or Groq if needed.
    """
    doc = nlp(query)
    concepts = []

    for token in doc:
        if token.dep_ in ("nsubj", "dobj", "attr", "pobj"):
            concepts.append(normalize_text(token.text))

    return concepts

def match_query_concepts_to_graph(query_concepts, graph_nodes, fuzzy_threshold=0.6):
    matched_concepts = []
    
    for concept in query_concepts:
        if concept in graph_nodes:
            matched_concepts.append(concept)
        else:
            # Fuzzy matching with a threshold
            closest_match = get_close_matches(concept, graph_nodes, n=1, cutoff=fuzzy_threshold)
            if closest_match:
                print(f"Fuzzy match found for '{concept}': '{closest_match[0]}'")
                matched_concepts.append(closest_match[0])
            else:
                print(f"No match found for: {concept}")
    return matched_concepts

def apply_ppr(graph, query_concepts, alpha=0.85):
    """
    Apply Personalized PageRank (PPR) to the graph using query concepts as seeds.
    """
    graph_nodes = [normalize_text(node) for node in graph.nodes()]
    print(f"Graph nodes: {graph_nodes}")  # Add this line to inspect the nodes
    
    matched_concepts = match_query_concepts_to_graph(query_concepts, graph_nodes)
    print("MATCHED CONCEPTS :" , matched_concepts)
    
    if not matched_concepts:
        raise ValueError(f"Query concepts {query_concepts} could not be matched to the graph nodes!")

   
    personalization = {node: 1 if node in matched_concepts else 0 for node in graph.nodes()}
    print("Personlization : ",personalization)
    print("==============================================================================================================================================================")
    try:
        scores = nx.pagerank(graph, alpha=alpha, personalization=personalization)
    except ZeroDivisionError:
        raise ValueError("Graph is disconnected, and PageRank cannot be applied!")
    
    print("Scores : ",scores)
    print("=================================================================================================================================================")
    return scores

def retrieve_subgraph(graph, scores, top_k=5):
    """
    Retrieve the top-k relevant subgraph based on PPR scores.
    """
    top_nodes = sorted(scores, key=scores.get, reverse=True)[:top_k]
    subgraph = graph.subgraph(top_nodes)
    return subgraph

def create_workflow_graph(workflow):
    workflow_graph = nx.DiGraph()

    # Extract nodes from workflow and add to NetworkX graph
    for node_name in workflow.nodes:
        # You can add custom attributes if needed, e.g., label or function name
        workflow_graph.add_node(node_name, label=node_name)

    # Extract edges from workflow and add to NetworkX graph
    for start_node, end_node in workflow.edges:
        workflow_graph.add_edge(start_node, end_node)
    
    return workflow_graph


def visualize_workflow_graph(graph, title="Workflow Graph"):
      pos = nx.spring_layout(graph, seed=42)
  
      edge_x, edge_y, edge_annotations = [], [], []
  
      for edge in graph.edges():
          x0, y0 = pos[edge[0]]
          x1, y1 = pos[edge[1]]
          edge_x.extend([x0, x1, None])
          edge_y.extend([y0, y1, None])
  
          edge_annotations.append(
              dict(
                  x=(x0 + x1) / 2,
                  y=(y0 + y1) / 2,
                  text="",
                  showarrow=False,
                  font=dict(size=10, color="blue"),
              )
          )
  
      node_x, node_y, node_labels = [], [], []
  
      for node in graph.nodes(data=True):
          x, y = pos[node[0]]
          node_x.append(x)
          node_y.append(y)
          node_labels.append(node[1].get("label", node[0]))
  
      edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(width=2, color="gray"), mode="lines")
  
      node_trace = go.Scatter(
          x=node_x,
          y=node_y,
          mode="markers+text",
          text=node_labels,
          textposition="top center",
          marker=dict(size=20, color="skyblue", line=dict(width=2, color="black")),
      )
  
      fig = go.Figure(
          data=[edge_trace, node_trace],
          layout=go.Layout(
              title=title,
              showlegend=False,
              hovermode="closest",
              xaxis=dict(showgrid=False, zeroline=False),
              yaxis=dict(showgrid=False, zeroline=False),
          ),
      )
  
      output_path = os.path.join(os.getcwd(), f"{title.replace(' ', '_').lower()}.html")
      fig.write_html(output_path)
      print(f"Workflow visualization saved at {output_path}")
      webbrowser.open("file://" + output_path)


def visualize_graph(graph, title="Knowledge Graph"):
    """
    Visualize the knowledge graph or subgraph using Plotly, showing relationships as edge labels.
    """
    # Generate positions for nodes using spring layout
    pos = nx.spring_layout(graph, seed=42)
    
    # Initialize lists for edges and nodes
    edge_x = []
    edge_y = []
    edge_labels = []
    edge_label_positions = []

    # Loop over the edges and extract x, y coordinates
    for edge in graph.edges(data=True):
        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        edge_x.extend([x0, x1, None])  # None for separating lines
        edge_y.extend([y0, y1, None])

        label = edge[2].get("label", "")  # Get the edge label if it exists
        edge_labels.append(label)
        edge_label_positions.append(((x0 + x1) / 2, (y0 + y1) / 2))  # Midpoint for label

    # Initialize lists for nodes
    node_x = []
    node_y = []
    node_text = []

    for node, (x, y) in pos.items():
        node_x.append(x)
        node_y.append(y)
        node_text.append(node)

    # Create edge trace (lines between nodes)
    edge_trace = go.Scatter(
        x=edge_x,
        y=edge_y,
        line=dict(width=1, color="gray"),
        hoverinfo="none",
        mode="lines",
    )

    # Create node trace (markers with text)
    node_trace = go.Scatter(
    x=node_x,
    y=node_y,
    mode="markers+text",
    text=node_text,
    textposition="top center",
    hoverinfo="text",  # Show only the node's text on hover
    marker=dict(
        showscale=True,
        colorscale="YlGnBu",
        size=20,
        colorbar=dict(
            thickness=15,
            title="Node Degree",
            xanchor="left",
            titleside="right",
        ),
      ),
    )
    # Create annotations for edge labels
    annotations = [
        dict(
            x=position[0],
            y=position[1],
            text=label,
            showarrow=False,
            font=dict(size=10, color="red"),
        )
        for position, label in zip(edge_label_positions, edge_labels)
    ]

    # Create Plotly figure
    fig = go.Figure(
        data=[edge_trace, node_trace],
        layout=go.Layout(
            title=title,
            showlegend=False,
            hovermode="closest",
            xaxis=dict(showgrid=False, zeroline=False),
            yaxis=dict(showgrid=False, zeroline=False),
            annotations=annotations,
        ),
    )

    # Output path for the graph HTML
    output_path = os.path.join(os.getcwd(), f"{title.replace(' ', '_').lower()}.html")
    
    # Write the figure to an HTML file
    fig.write_html(output_path)
    print(f"Graph visualization has been saved to {output_path}")
    
    # Open the HTML file in the browser
    webbrowser.open("file://" + output_path)

def generate_augmented_response(query, context_str):
    """
    Generate a response using Groq LLM by providing context and a query.
    """
    llm = ChatGroq(
        model="mixtral-8x7b-32768",  
        temperature=0.5,
        max_tokens=1024,
    )

    prompt = f"""
    You are an assistant that provides comprehensive answers by analyzing and synthesizing information from the given context. 
    The context provided is the graph data like nodes and edges which are connecting to some information . So you have to generate a repsonse on this given context and dont tell that this node or this is edge on those nodes and edges generate a response Context: {context_str}
    Answer the following question using the provided context: {query}
    """

    response = llm.invoke(prompt)
    
    return response.content

def extract_textual_subgraph_data(subgraph):
    """
    Convert the subgraph into a textual string representing the nodes and edges.
    """
    context_str = ""
    print(f"Subgraph ke nodes : {subgraph.nodes()}")
    for node in subgraph.nodes():
        neighbors = list(subgraph.neighbors(node))
        context_str += f"Node: {node} -> Neighbors: {', '.join(map(str, neighbors))}\n"
        
        for neighbor in neighbors:
            edge_data = subgraph.get_edge_data(node, neighbor)  
            context_str += f"Edge: {node} -> {neighbor} with data: {edge_data}\n"
    
    return context_str

def load_pdf_with_langchain(pdf_path, chunk_size=1000, overlap=50):
    try:
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()
        text = "\n".join(doc.page_content for doc in documents)
        
        # Use RecursiveCharacterTextSplitter to split the text
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)
        chunks = text_splitter.split_text(text)
        print("CHUNKS : ",chunks)
        return chunks
    except Exception as e:
        print(f"Error reading PDF with LangChain: {e}")
        return []
    

# if __name__ == "__main__":
#     parser = argparse.ArgumentParser()
#     parser.add_argument("--query", type=str, help="Query to process with the knowledge graph")
#     parser.add_argument("--top_k", type=int, default=5, help="Number of nodes to retrieve in the subgraph")
#     args = parser.parse_args()

#     # Example corpus (for testing purposes)
#     pdf_path = "C:/Users/Coditas-Admin/Desktop/POC HIPPO RAG/Hippo/Basic KG/Python Data.pdf"
#     text_chunks = load_pdf_with_langchain(pdf_path)

#     all_triples = []

# # Step 1: Process each chunk 
#     for i, chunk in enumerate(text_chunks):
#         print(f"Processing chunk {i+1}/{len(text_chunks)}...")
        
#        # Step 1: Convert corpus to triples using LLM
#         try:
#             triples = transform_corpus_to_knowledge_graph(chunk)
#             print(f"Chunk {i+1} triples:", triples)
#             all_triples.extend(triples)
#         except Exception as e:
#             print(f"Error processing chunk {i+1}: {e}")
    
#     print("Triplets :",all_triples)
#     print("==============================================================================================================================================")
    
#     # triples = transform_corpus_to_knowledge_graph(TEXT_DATA)
#     # print("Triplets :",triples)
#     # print("==============================================================================================================================================================") 

#     # Step 2: Build Knowledge Graph from triples
#     knowledge_graph = build_knowledge_graph_from_llm(all_triples)
#     print("KNOWLEDGE GRAPH : ",knowledge_graph)
#     visualize_graph(knowledge_graph , title="Whole Knowledge Graph")

#     # Step 3: Extract query concepts
#     query_concepts = extract_query_concepts(args.query)
#     print(f"Query concepts: {query_concepts}")
#     print("=====================================================================================================================================================")

#     # Step 4: Apply Personalized PageRank
#     ppr_scores = apply_ppr(knowledge_graph, query_concepts)

#     # Step 5: Retrieve the top-K relevant subgraph
#     subgraph = retrieve_subgraph(knowledge_graph, ppr_scores, top_k=args.top_k)
#     visualize_graph(subgraph, title="Relevant Subgraph")

#     # Step 6: Generate the textual data for the subgraph
#     context_data = extract_textual_subgraph_data(subgraph)
#     print("Subgraph :" , context_data)
#     print("=======================================================================================================================================================")

#     # Step 7: Generate a response based on the query and context
#     response = generate_augmented_response(args.query, context_data)
#     print("Response:", response)
